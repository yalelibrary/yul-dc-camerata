Notes:

  1. you must use platform version 1.4.0, the default and (ironically) 
  LATEST version referred to in the documentation is 1.3.5, this won't work.
  At some point LATEST will point to 1.4.0, and this won't be an issue.

  The pre-release version of ecs-ci that we use has a known issue in their
  tracker that it specifies the wrong platform version when efs volumes are
  detected.  For now, the workaround is to upgrade in the aws console (goto
  cluster->update cluster, select platform version 1.4.0 and click through the
  rest. This only has to be done once. Verify things happened by noting the
  platform version displayed in the task section of the cluster screen.

  speaking of pre-release ecs-ci, you need to use something from the 1.19.x
  stream. I built mine from scratch  (set up a $GOPATH, 'go get
  github.com/aws/amazon-ecs-cli', figure out where in the gopath that got
  checked out to, and run the included build script, copy the fresh binary
  into your path)

  2. you have to set up efs client access policy, a sample is included,
  and should be applied to the efs resource (there's a tiny "manage client
  access" link on its details page in the efs management console).  This will
  fix "server access denied" errors.  In the future, we should be using efs
  access points, but i haven't been able to get them working correctly yet.
  the access policy allows access from any aws principal, which should probably 
  be trimmed down, but i'm not a wizard.
  https://docs.aws.amazon.com/efs/latest/ug/create-file-system-policy.html


  3. now access points are working.  You will need 2, and they should be named
  $cluster-solr and $cluster-psql, so to integrate, we will need to create
  those and extract the resource id (this should only need to be done once,
  so, maybe in the create script.  This will allow us to segment a single efs
  filesystem into as many pieces as we want. Now that i'm writing this down,
  it occurrs to me that each cluster has its own vpc, the efs filesystem lives
  there... 

  To configure an access point, you need the numeric user ids of the users
  accessing the filesystem. the solr user is 8983, and postgres is 999. The
  path setting should be $clustername-psql and $clustername-solr.  This will
  "alias" the root for clients (the client can't tell they are using a
  subdirectory, and not the root when mounting). This allows us to segment the
  filesystems that various containers need (for instance, we could have
  multiple dbs, without having to fiddle with the $DATA filesystem).

  filesystem creation reference:
  https://docs.aws.amazon.com/efs/latest/ug/creating-using-create-fs.html
  https://docs.aws.amazon.com/efs/latest/ug/create-access-point.html





